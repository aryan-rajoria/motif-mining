{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045459de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import models\n",
    "\n",
    "from torchvision.transforms import Compose, transforms\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59e3d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717bab6",
   "metadata": {},
   "source": [
    "### Loading Model and getting inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3036cf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kravicha3/aryan/env/bulk/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kravicha3/aryan/env/bulk/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True, progress=False)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = torch.nn.Identity()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad56e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_numpy(val: Tensor) -> np.ndarray:\n",
    "        return val.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f07717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(images: np.ndarray):\n",
    "    transformed = [transforms.ToTensor()]\n",
    "    composed = Compose(transformed)\n",
    "    return composed(Image.fromarray(images[:, :, ::-1])).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557a94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HOME = \"/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Cat_flowing_down_a_sofa/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac6523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for file in os.listdir(IMAGE_HOME):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494716ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g054_dgzi4ym.jpg', 'g054_dgzoaw3.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b3114d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = IMAGE_HOME + \"g054_dgzg3g2.jpg\"\n",
    "img = cv2.imread(IMAGE_PATH)\n",
    "imgt = transform(img)\n",
    "# f = (3, width, height) values: 0-1\n",
    "imgt = imgt.to(device)\n",
    "with torch.no_grad():\n",
    "    inference = as_numpy(model(torch.unsqueeze(imgt[0], 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "737a684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048) float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2048)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inference.shape, inference.dtype)\n",
    "inference.reshape(1, -1)\n",
    "inference.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da2ea4",
   "metadata": {},
   "source": [
    "### Getting Faiss scan result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf613da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATH =  \"/nethome/kravicha3/.eva/0.1.5+dev/index/HNSW_dataindex.index\"\n",
    "index = faiss.read_index(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1cee65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "D, I = index.search(inference, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc6bb4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.0,\n",
       "   71.97640228271484,\n",
       "   81.04901885986328,\n",
       "   81.88858795166016,\n",
       "   83.09184265136719,\n",
       "   83.36123657226562,\n",
       "   84.52255249023438,\n",
       "   87.03155517578125,\n",
       "   87.73091125488281,\n",
       "   89.1760025024414]],\n",
       " [[9079, 8710, 7866, 2058, 2057, 7965, 2207, 4820, 4627, 2132]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = D.tolist()\n",
    "I = I.tolist()\n",
    "D,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0764789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9079, 0.0)\n",
      "(8710, 71.97640228271484)\n",
      "(7866, 81.04901885986328)\n",
      "(2058, 81.88858795166016)\n",
      "(2057, 83.09184265136719)\n",
      "(7965, 83.36123657226562)\n",
      "(2207, 84.52255249023438)\n",
      "(4820, 87.03155517578125)\n",
      "(4627, 87.73091125488281)\n",
      "(2132, 89.1760025024414)\n"
     ]
    }
   ],
   "source": [
    "for i in list(zip(I[0], D[0])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfdf027",
   "metadata": {},
   "source": [
    "## Checking results using sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c61268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect(\"/home/kravicha3/.eva/0.1.5+dev/eva_catalog.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ceeddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe4e6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_This_cat_plotting_to_kill_someone/g1327_czcqbl6.jpg')\n",
      "\n",
      "(2, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_This_cat_plotting_to_kill_someone/g1327_czcu1y7.jpg')\n",
      "\n",
      "(3, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_This_cat_plotting_to_kill_someone/g1327_czd2m0n.jpg')\n",
      "\n",
      "(4, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_This_cat_plotting_to_kill_someone/g1327_czcrc83.png')\n",
      "\n",
      "(5, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_This_cat_plotting_to_kill_someone/g1327_czd40us.jpg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT * FROM '192111ccbbbfc5042415841dfaa9f90a' LIMIT 5;\")\n",
    "r = c.fetchall()\n",
    "for i in r:\n",
    "    print(i, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31a9b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9079, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Cat_flowing_down_a_sofa/g054_dgzg3g2.jpg')]\n",
      "[(8710, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_a_frog_riding_a_beetle/g382_d13crsr.jpg')]\n",
      "[(7866, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Alexis_Ohanian_(CEO_and_founder_of_reddit)_holding_a_sign/g1333_cnorg0z.jpg')]\n",
      "[(2058, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Vladimir_Putin_in_a_submarine_in_the_Black_Sea/g1191_cu7aerv.jpg')]\n",
      "[(2057, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Vladimir_Putin_in_a_submarine_in_the_Black_Sea/g1191_cu794hl.jpg')]\n",
      "[(7965, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Hawk_Owl_flying,_looking_into_camera/g1294_cvlk0dk.jpg')]\n",
      "[(2207, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_This_man_dancing_at_a_wedding_(x-post_from__r_pics)/g097_dcl76ds.jpg')]\n",
      "[(4820, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Man_in_T-Rex_costume_crying_out_in_rain/g010_dhb1xk6.png')]\n",
      "[(4627, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_Peter_Dinklage_riding_a_scooter/g186_d4k5o86.jpg')]\n",
      "[(2132, '/nethome/kravicha3/aryan/project/dataset/Reddit_Provenance_Datasets/data/_An_angry_dog_standing_upright/g980_cqgiq8a.png')]\n"
     ]
    }
   ],
   "source": [
    "for i in I[0]:\n",
    "    c.execute(f\"SELECT * FROM '192111ccbbbfc5042415841dfaa9f90a' WHERE _row_id={i}\")\n",
    "    r= c.fetchall()\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595f92a",
   "metadata": {},
   "source": [
    "## Changing FAISS to OPQ and IVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "29ab7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2048\n",
    "code_size = 32 # bytes\n",
    "ncentroids = 512\n",
    "\n",
    "coarse_quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFPQ (coarse_quantizer, d,\n",
    "                          512, code_size, 8)\n",
    "index.nprobe = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "132c6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_indexing(start_path = '.'):\n",
    "    number_of_files = 0\n",
    "    numpy_array = None\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            \n",
    "            if not os.path.islink(fp):\n",
    "                if number_of_files!=0:\n",
    "                    img = cv2.imread(fp)\n",
    "                    if img is not None:\n",
    "                        imgt = transform(img)\n",
    "                        imgt = imgt.to(device)\n",
    "                        with torch.no_grad():\n",
    "                            inference = as_numpy(model(torch.unsqueeze(imgt[0], 0)))\n",
    "                        numpy_array = np.append(numpy_array, inference, axis=0)\n",
    "                        number_of_files += 1\n",
    "                        break_flag = True\n",
    "                else:\n",
    "                    img = cv2.imread(fp)\n",
    "                    imgt = transform(img)\n",
    "                    imgt = imgt.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        inference = as_numpy(model(torch.unsqueeze(imgt[0], 0)))\n",
    "                    numpy_array = inference\n",
    "                    number_of_files += 1\n",
    "        if number_of_files>512:\n",
    "            break\n",
    "\n",
    "    return numpy_array, number_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1f70e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n"
     ]
    }
   ],
   "source": [
    "array, num = run_indexing('../../dataset/Reddit_Provenance_Datasets/data/')\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "44c0b6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(555, 2048)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7e70a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "46c91f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 555 points to 512 centroids: please provide at least 19968 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 555 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    }
   ],
   "source": [
    "index.train(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b30f8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea443f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
